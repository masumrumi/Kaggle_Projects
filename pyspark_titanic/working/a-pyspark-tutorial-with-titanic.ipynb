{"cells":[{"cell_type":"markdown","metadata":{},"source":"This kernel will give a tutorial for starting out with PySpark using Titanic dataset. Let's get started. \n\n\n### Kernel Goals\n<a id=\"aboutthiskernel\"></a>\n***\nThere are three primary goals of this kernel.\n- <b>Provide a tutorial for someone who is starting out with pyspark.\n- <b>Do an exploratory data analysis(EDA)</b> of titanic with visualizations and storytelling.  \n- <b>Predict</b>: Use machine learning classification models to predict the chances of passengers survival.\n\n### What is Spark, anyway?\nSpark is a platform for cluster computing. Spark lets us spread data and computations over clusters with multiple nodes (think of each node as a separate computer). Splitting up data makes it easier to work with very large datasets because each node only works with a small amount of data.\nAs each node works on its own subset of the total data, it also carries out a part of the total calculations required, so that both data processing and computation are performed in parallel over the nodes in the cluster. It is a fact that parallel computation can make certain types of programming tasks much faster.\n\nDeciding whether or not Spark is the best solution for your problem takes some experience, but you can consider questions like:\n* Is my data too big to work with on a single machine?\n* Can my calculations be easily parallelized?\n\n"},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('../input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"},"outputs":[],"source":"## installing pyspark\n!pip install pyspark"},{"cell_type":"markdown","metadata":{},"source":"The first step in using Spark is connecting to a cluster. In practice, the cluster will be hosted on a remote machine that's connected to all other nodes. There will be one computer, called the master that manages splitting up the data and the computations. The master is connected to the rest of the computers in the cluster, which are called worker. The master sends the workers data and calculations to run, and they send their results back to the master.\n\nWe definitely don't need may clusters for Titanic dataset. In addition to that, the syntax for running locally or using many clusters are pretty similar. To start working with Spark DataFrames, we first have to create a SparkSession object from SparkContext. We can think of the SparkContext as the connection to the cluster and SparkSession as the interface with that connection. Let's create a SparkSession. "},{"cell_type":"markdown","metadata":{},"source":"# Beginner Tutorial\nThis part is solely for beginners. I recommend starting from to get a good understanding of the flow. "},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":"## creating a spark session\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('tutorial').getOrCreate()"},{"cell_type":"markdown","metadata":{},"source":"Let's read the dataset. "},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":"df = spark.read.csv('../input/titanic/train.csv', header = True, inferSchema=True)\ndf_test = spark.read.csv('../input/titanic/test.csv', header = True, inferSchema=True)"},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":"## So, what is df?\ntype(df)"},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":"## As you can see it's a Spark dataframe. Let's take a look at the preview of the dataset. \ndf.show()"},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":"## It looks a bit messi. See what I did there? ;). Anyway, how about using .toPandas() for change. \ndf.toPandas()"},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":"# I use the toPandas() in a riddiculous amount as you will see in this kernel. \n# It is just convenient and doesn't put a lot of constran in my eye. \n## in addition to that if you know pandas, this can be very helpful \n## for checking your work.\n## how about a summary. \nresult = df.describe().toPandas()"},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":"result"},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":"# We can also convert a pandas dataframe to spark dataframe. Here is how we do it. \nprint(f\"Before: {type(result)}\")\nspark_temp = spark.createDataFrame(result)\nprint(f\"After: {type(spark_temp)}\")"},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":"# Cool, Let's print the schema of the df using .printSchema()\ndf.printSchema()"},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":"# similar approach\ndf.dtypes"},{"cell_type":"markdown","metadata":{},"source":"The data in the real world is not this clean. We often have to create our own schema and implement it. We will describe more about it in the future. Since we are talking about schema, are you wondering if you would be able to implement sql with Spark?. Yes, you can. \n\nOne of the best advantage of Spark is that you can run sql commands to do analysis. If you are like that nifty co-worker of mine, you would probably want to use sql with spark. Let's do an example. "},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":"## First, we need to register a sql temporary view.\ndf.createOrReplaceTempView(\"mytable\");\n\n## Then, we use spark.sql and write sql inside it. \nresult = spark.sql(\"SELECT * FROM mytable ORDER BY Fare DESC LIMIT 10\")\nresult.toPandas()"},{"cell_type":"markdown","metadata":{},"source":"Similarly we can also register another sql temp view. "},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":"df_test.createOrReplaceTempView(\"df_test\")"},{"cell_type":"markdown","metadata":{},"source":"Now that we have registered two tables with in this spark session, wondering how we can see which once are registered?"},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":"spark.catalog.listTables()"},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":"# We can also create spark dataframe out of these tables, Here is how we do it.\ntemp_table = spark.table(\"df_test\")\nprint(type(temp_table))\ntemp_table.show(5)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":"# pretty cool, We will dive deep in sql later. \n# Let's go back to dataFrame and do some nitty-gritty stuff. \n# What if want the column names only. \ndf.columns"},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":"# What about just a column?\ndf['Age']"},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":"type(df['Age'])"},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":"# Well, that's not what we pandas users have expected. \n# Yes, in order to get a column we need to use select().  \n# df.select(df['Age']).show()\ndf.select('Age').show()"},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":"## What if we want multiple columns?\ndf.select(['Age', 'Fare']).show()"},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":"# that's more like it. \n# What about accessing a row\ndf.head(1)"},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":"type(df.head(1))"},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":"## returns a list. let's get the item in the list\nrow = df.head(1)[0]\nrow"},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":"type(row)"},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":"## row can be converted into dict using .asDict()\nrow.asDict()"},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":"## Then the value can be accessed from the row dictionaly. \nrow.asDict()['PassengerId']"},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":"## similarly\nrow.asDict()['Name']"},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":"## let's say we want to change the name of a column. we can use withColumnRenamed\n# df.withColumnRenamed('exsisting name', 'anticipated name');\ndf.withColumnRenamed(\"Age\", \"newA\").limit(5).toPandas()"},{"cell_type":"code","execution_count":30,"metadata":{"scrolled":false},"outputs":[],"source":"# Let's say we want to modify a column, for example, add in this case, adding $20 with every fare. \n## df.withColumn('existing column', 'calculation with the column(we have to put df not just column)')\n## so not df.withColumn('Fare', 'Fare' +20).show()\ndf.withColumn('Fare', df['Fare']+20).limit(5).show()"},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":"## let's say we want get the average fare.\n# we will use the \"mean\" function from pyspark.sql.functions(this is where all the functions are stored) and\n# collect the data using \".collect()\" instead of using .show()\n# collect returns a list so we need to get the value from the list using index"},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":"from pyspark.sql.functions import mean\nfare_mean = df.select(mean(\"Fare\")).collect()\nfare_mean[0][0]"},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":"fare_mean = fare_mean[0][0]\nfare_mean"},{"cell_type":"markdown","metadata":{},"source":"#### Filter"},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":"# What if we want to filter data and see all datapoints above average. \n# there are two approaches of this, we can use sql syntex/passing a string\n# or just dataframe approach. \ndf.filter(\"Fare > 32.20\" ).limit(3).show()"},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":"# or we can use the dataframe approach\ndf.filter(df['Fare']> fare_mean).limit(3).show()"},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":"## What if we want to filter by multiple columns.\n# passenger with below average fare with a Pclass equals 3\ndf.filter((df['Fare'] < fare_mean) &\n          (df['Pclass'] ==  3)\n         ).show(10)"},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":"# passenger with below fare and are not male\nfilter1_less_than_mean_fare = df['Fare'] < fare_mean\nfilter2_sex_not_male = df['Sex'] != \"male\"\ndf.filter((filter1_less_than_mean_fare) &\n          (filter2_sex_not_male)).show(10)"},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":"# We can also apply it this way\n# passenger with below fare and are not male\n# creating filters\nfilter1_less_than_mean_fare = df['Fare'] < fare_mean\nfilter2_sex_not_male = df['Sex'] != \"male\"\n# applying filters\ndf.filter(filter1_less_than_mean_fare).filter(filter2_sex_not_male).show(10)"},{"cell_type":"markdown","metadata":{},"source":"#### GroupBy"},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":"## Let's group by Pclass and see how the average fare price. \ndf.groupBy(\"Pclass\").mean().toPandas()"},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":"## let's just look at the Pclass and avg(Fare)\ndf.groupBy(\"Pclass\").mean().select('Pclass', 'avg(Fare)').show()"},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":"# Alternative way\ndf.groupBy(\"Pclass\").mean(\"Fare\").show()"},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":"## What if we want just the average of all fare, we can use .agg with the dataframe. \ndf.agg({'Fare':'mean'}).show()"},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":"## another way this can be done is by importing \"mean\" funciton from pyspark.sql.functions\nfrom pyspark.sql.functions import mean\ndf.select(mean(\"Fare\")).show()"},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":"## we can also combine the few previous approaches to get similar results. \ntemp = df.groupBy(\"Pclass\")\ntemp.agg({\"Fare\": 'mean'}).show()"},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":"# What if we want to format the results. \n# for example,\n# I want to rename the column. this will be accomplished using .alias() method.  \n# I want to format the number with only two decimals. this can be done using \"format_number\"\nfrom pyspark.sql.functions import format_number\ntemp = df.groupBy(\"Pclass\")\ntemp = temp.agg({\"Fare\": 'mean'})\ntemp.select('Pclass', format_number(\"avg(Fare)\", 2).alias(\"average fare\")).show()"},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":"## What if I want to order by Fare in ascending order. \ndf.orderBy(\"Fare\").limit(20).toPandas()"},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":"## What about descending order\ndf.orderBy(df['Fare'].desc()).limit(5).toPandas()"},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[],"source":"## How do we deal with missing values. \n# df.na.drop(how=(\"any\"/\"all\"), thresh=(1,2,3,4,5...))\ndf.na.drop(how=\"any\").limit(5).toPandas()"},{"cell_type":"markdown","metadata":{},"source":"# Advanced Tutorial\n## Dealing with Missing Values\n### Cabin"},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":"# filling the null values in cabin with \"N\".\n# df.fillna(value, subset=[]);\ndf = df.na.fill('N', subset=['Cabin'])\ndf_test = df_test.na.fill('N', subset=['Cabin'])"},{"cell_type":"markdown","metadata":{},"source":"### Fare"},{"cell_type":"code","execution_count":50,"metadata":{"scrolled":true},"outputs":[],"source":"## how do we find out the rows with missing values?\n# we can use .where(condition) with .isNull()\ndf_test.where(df_test['Fare'].isNull()).show()"},{"cell_type":"markdown","metadata":{},"source":"Here, We can take the average of the **Fare** column to fill in the NaN value. However, for the sake of learning and practicing, we will try something else. We can take the average of the values where **Pclass** is ***3***, **Sex** is ***male*** and **Embarked** is ***S***"},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":"missing_value = df_test.filter(\n    (df_test['Pclass'] == 3) &\n    (df_test.Embarked == 'S') &\n    (df_test.Sex == \"male\")\n)\n## filling in the null value in the fare column using Fare mean. \ndf_test = df_test.na.fill(\n    missing_value.select(mean('Fare')).collect()[0][0],\n    subset=['Fare']\n)"},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":"# Checking\ndf_test.where(df_test['Fare'].isNull()).show()"},{"cell_type":"markdown","metadata":{},"source":"### Embarked"},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":"df.where(df['Embarked'].isNull()).show()"},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[],"source":"## Replacing the null values in the Embarked column with the mode. \ndf = df.na.fill('C', subset=['Embarked'])"},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":"## checking\ndf.where(df['Embarked'].isNull()).show()"},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":"df_test.where(df_test.Embarked.isNull()).show()"},{"cell_type":"markdown","metadata":{},"source":"## Feature Engineering\n### Cabin"},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[],"source":"## this is a code to create a wrapper for function, that works for both python and Pyspark.\nfrom typing import Callable\nfrom pyspark.sql import Column\nfrom pyspark.sql.functions import udf, col\nfrom pyspark.sql.types import StringType, IntegerType, ArrayType, DataType\nclass py_or_udf:\n    def __init__(self, returnType : DataType=StringType()):\n        self.spark_udf_type = returnType\n        \n    def __call__(self, func : Callable):\n        def wrapped_func(*args, **kwargs):\n            if any([isinstance(arg, Column) for arg in args]) or \\\n                any([isinstance(vv, Column) for vv in kwargs.values()]):\n                return udf(func, self.spark_udf_type)(*args, **kwargs)\n            else:\n                return func(*args, **kwargs)\n        return wrapped_func\n\n    \n@py_or_udf(returnType=StringType())\ndef first_char(col):\n    return col[0]\n    "},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[],"source":"df = df.withColumn('Cabin', first_char(df['Cabin']))"},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[],"source":"df_test = df_test.withColumn('Cabin', first_char(df_test['Cabin']))"},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[],"source":"df.limit(5).toPandas()"},{"cell_type":"markdown","metadata":{},"source":"We can use the average of the fare column We can use pyspark's ***groupby*** function to get the mean fare of each cabin letter."},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":"df.groupBy('Cabin').mean(\"Fare\").show()"},{"cell_type":"markdown","metadata":{},"source":"Now, these mean can help us determine the unknown cabins, if we compare each unknown cabin rows with the given mean's above. Let's write a simple function so that we can give cabin names based on the means. "},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[],"source":"@py_or_udf(returnType=StringType())\ndef cabin_estimator(i):\n    \"\"\"Grouping cabin feature by the first letter\"\"\"\n    a = 0\n    if i<16:\n        a = \"G\"\n    elif i>=16 and i<27:\n        a = \"F\"\n    elif i>=27 and i<38:\n        a = \"T\"\n    elif i>=38 and i<47:\n        a = \"A\"\n    elif i>= 47 and i<53:\n        a = \"E\"\n    elif i>= 53 and i<54:\n        a = \"D\"\n    elif i>=54 and i<116:\n        a = 'C'\n    else:\n        a = \"B\"\n    return a"},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":"## separating data where Cabin == 'N', remeber we used 'N' for Null. \ndf_withN = df.filter(df['Cabin'] == 'N')\ndf2 = df.filter(df['Cabin'] != 'N')\n\n## replacing 'N' using cabin estimated function. \ndf_withN = df_withN.withColumn('Cabin', cabin_estimator(df_withN['Fare']))\n\n# putting the dataframe back together. \ndf = df_withN.union(df2).orderBy('PassengerId') "},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[],"source":"#let's do the same for test set\ndf_testN = df_test.filter(df_test['Cabin'] == 'N')\ndf_testNoN = df_test.filter(df_test['Cabin'] != 'N')\ndf_testN = df_testN.withColumn('Cabin', cabin_estimator(df_testN['Fare']))\ndf_test = df_testN.union(df_testNoN).orderBy('PassengerId')"},{"cell_type":"markdown","metadata":{},"source":"### Name"},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[],"source":"## creating UDF functions\n@py_or_udf(returnType=IntegerType())\ndef name_length(name):\n    return len(name)\n\n\n@py_or_udf(returnType=StringType())\ndef name_length_group(size):\n    a = ''\n    if (size <=20):\n        a = 'short'\n    elif (size <=35):\n        a = 'medium'\n    elif (size <=45):\n        a = 'good'\n    else:\n        a = 'long'\n    return a"},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[],"source":"## getting the name length from name. \ndf = df.withColumn(\"name_length\", name_length(df['Name']))\n\n## grouping based on name length. \ndf = df.withColumn(\"nLength_group\", name_length_group(df['name_length']))"},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[],"source":"## Let's do the same for test set. \ndf_test = df_test.withColumn(\"name_length\", name_length(df_test['Name']))\n\ndf_test = df_test.withColumn(\"nLength_group\", name_length_group(df_test['name_length']))"},{"cell_type":"markdown","metadata":{},"source":"### Title"},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[],"source":"## this function helps getting the title from the name. \n@py_or_udf(returnType=StringType())\ndef get_title(name):\n    return name.split('.')[0].split(',')[1].strip()\n\ndf = df.withColumn(\"title\", get_title(df['Name']))\ndf_test = df_test.withColumn('title', get_title(df_test['Name']))"},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[],"source":"## we are writing a function that can help us modify title column\n@py_or_udf(returnType=StringType())\ndef fuse_title1(feature):\n    \"\"\"\n    This function helps modifying the title column\n    \"\"\"\n    if feature in ['the Countess','Capt','Lady','Sir','Jonkheer','Don','Major','Col', 'Rev', 'Dona', 'Dr']:\n        return 'rare'\n    elif feature in ['Ms', 'Mlle']:\n        return 'Miss'\n    elif feature == 'Mme':\n        return 'Mrs'\n    else:\n        return feature"},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[],"source":"df = df.withColumn(\"title\", fuse_title1(df[\"title\"]))"},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[],"source":"df_test = df_test.withColumn(\"title\", fuse_title1(df_test['title']))"},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[],"source":"print(df.toPandas()['title'].unique())\nprint(df_test.toPandas()['title'].unique())"},{"cell_type":"markdown","metadata":{},"source":"### family_size"},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[],"source":"df = df.withColumn(\"family_size\", df['SibSp']+df['Parch'])\ndf_test = df_test.withColumn(\"family_size\", df_test['SibSp']+df_test['Parch'])"},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[],"source":"## bin the family size. \n@py_or_udf(returnType=StringType())\ndef family_group(size):\n    \"\"\"\n    This funciton groups(loner, small, large) family based on family size\n    \"\"\"\n    \n    a = ''\n    if (size <= 1):\n        a = 'loner'\n    elif (size <= 4):\n        a = 'small'\n    else:\n        a = 'large'\n    return a"},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[],"source":"df = df.withColumn(\"family_group\", family_group(df['family_size']))\ndf_test = df_test.withColumn(\"family_group\", family_group(df_test['family_size']))\n"},{"cell_type":"markdown","metadata":{},"source":"### is_alone"},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[],"source":"@py_or_udf(returnType=IntegerType())\ndef is_alone(num):\n    if num<2:\n        return 1\n    else:\n        return 0"},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[],"source":"df = df.withColumn(\"is_alone\", is_alone(df['family_size']))\ndf_test = df_test.withColumn(\"is_alone\", is_alone(df_test[\"family_size\"]))"},{"cell_type":"markdown","metadata":{},"source":"### ticket"},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[],"source":"## dropping ticket column\ndf = df.drop('ticket')\ndf_test = df_test.drop(\"ticket\")"},{"cell_type":"markdown","metadata":{},"source":"### calculated_fare"},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[],"source":"from pyspark.sql.functions import expr, col, when, coalesce, lit"},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[],"source":"## here I am using a something similar to if and else statement, \n#when(condition, value_when_condition_met).otherwise(alt_condition)\ndf = df.withColumn(\n    \"calculated_fare\", \n    when((col(\"Fare\")/col(\"family_size\")).isNull(), col('Fare'))\n    .otherwise((col(\"Fare\")/col(\"family_size\"))))"},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[],"source":"df_test = df_test.withColumn(\n    \"calculated_fare\", \n    when((col(\"Fare\")/col(\"family_size\")).isNull(), col('Fare'))\n    .otherwise((col(\"Fare\")/col(\"family_size\"))))"},{"cell_type":"markdown","metadata":{},"source":"### fare_group"},{"cell_type":"code","execution_count":82,"metadata":{},"outputs":[],"source":"@py_or_udf(returnType=StringType())\ndef fare_group(fare):\n    \"\"\"\n    This function creates a fare group based on the fare provided\n    \"\"\"\n    \n    a= ''\n    if fare <= 4:\n        a = 'Very_low'\n    elif fare <= 10:\n        a = 'low'\n    elif fare <= 20:\n        a = 'mid'\n    elif fare <= 45:\n        a = 'high'\n    else:\n        a = \"very_high\"\n    return a"},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[],"source":"df = df.withColumn(\"fare_group\", fare_group(col(\"Fare\")))\ndf_test = df_test.withColumn(\"fare_group\", fare_group(col(\"Fare\")))"},{"cell_type":"markdown","metadata":{},"source":"# That's all for today. Let's come back tomorrow when we will learn how to apply machine learning with Pyspark"},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[],"source":"# Binarizing, Bucketing & Encoding"},{"cell_type":"code","execution_count":85,"metadata":{},"outputs":[],"source":"train = spark.read.csv('../input/titanic/train.csv', header = True, inferSchema=True)\ntest = spark.read.csv('../input/titanic/test.csv', header = True, inferSchema=True)"},{"cell_type":"code","execution_count":86,"metadata":{},"outputs":[],"source":"train.show()"},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[],"source":"# Binarzing\nfrom pyspark.ml.feature import Binarizer\n# Cast the data type to double\ntrain = train.withColumn('SibSp', train['SibSp'].cast('double'))\n# Create binarzing transform\nbin = Binarizer(threshold=0.0, inputCol='SibSp', outputCol='SibSpBin')\n# Apply the transform\ntrain = bin.transform(train)"},{"cell_type":"code","execution_count":88,"metadata":{},"outputs":[],"source":"train.select('SibSp', 'SibSpBin').show(10)"},{"cell_type":"code","execution_count":89,"metadata":{},"outputs":[],"source":"# Bucketing\nfrom pyspark.ml.feature import Bucketizer\n# We are going to bucket the fare column\n# Define the split\nsplits = [0,4,10,20,45, float('Inf')]\n\n# Create bucketing transformer\nbuck = Bucketizer(splits=splits, inputCol='Fare', outputCol='FareB')\n\n# Apply transformer\ntrain = buck.transform(train)"},{"cell_type":"code","execution_count":90,"metadata":{},"outputs":[],"source":"train.toPandas().head(10)"},{"cell_type":"code","execution_count":91,"metadata":{},"outputs":[],"source":"# One Hot Encoding\n# it is a two step process\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer\n# Create indexer transformer for Sex Column\n\n# Step 1: Create indexer for texts\nstringIndexer = StringIndexer(inputCol='Sex', outputCol='SexIndex')\n\n# fit transform\nmodel = stringIndexer.fit(train)\n\n# Apply transform\nindexed = model.transform(train)"},{"cell_type":"code","execution_count":92,"metadata":{},"outputs":[],"source":"# Step 2: One Hot Encode\n# Create encoder transformer\nencoder = OneHotEncoder(inputCol='SexIndex', outputCol='Sex_Vec')\n\n# fit model\nmodel = encoder.fit(indexed)\n\n# apply transform\nencoded_df = model.transform(indexed)\n\nencoded_df.toPandas().head()"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""},{"cell_type":"markdown","metadata":{},"source":"<div class=\"alert alert-info\">\n    <h1>Resources</h1>\n    <ul>\n        <li><a href=\"https://docs.databricks.com/spark/latest/spark-sql/udf-python.html\">User-defined functions - Python</a></li>\n        <li><a href=\"https://medium.com/@ayplam/developing-pyspark-udfs-d179db0ccc87\">Developing PySpark UDFs</a></li>\n    </ul>\n        <h1>Credits</h1>\n    <ul>\n        <li>To DataCamp, I have learned so much from DataCamp.</li>\n        <li>To Jose Portilla, Such an amazing teacher with all of his resources</li>\n    </ul>\n    \n</div>"},{"cell_type":"markdown","metadata":{},"source":"<div class=\"alert alert-info\">\n<h4>If you like to discuss any other projects or just have a chat about data science topics, I'll be more than happy to connect with you on:</h4>\n    <ul>\n        <li><a href=\"https://www.linkedin.com/in/masumrumi/\"><b>LinkedIn</b></a></li>\n        <li><a href=\"https://github.com/masumrumi\"><b>Github</b></a></li>\n        <li><a href=\"https://masumrumi.com/\"><b>masumrumi.com</b></a></li>\n    </ul>\n\n<p>This kernel will always be a work in progress. I will incorporate new concepts of data science as I comprehend them with each update. If you have any idea/suggestions about this notebook, please let me know. Any feedback about further improvements would be genuinely appreciated.</p>\n\n<h1>If you have come this far, Congratulations!!</h1>\n\n<h1>If this notebook helped you in any way or you liked it, please upvote and/or leave a comment!! :)</h1></div>"},{"cell_type":"markdown","metadata":{},"source":"<div class=\"alert alert-info\">\n    <h1>Versions</h1>\n    <ul>\n        <li>Version 16</li>\n    </ul>\n    \n</div>"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":4}